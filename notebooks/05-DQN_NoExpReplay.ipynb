{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from insoco.Environment import Environment, ACT_MODE, OBS_MODE\n",
    "from insoco.Plotting import plotQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_function():  \n",
    "    # stores the Q_function Q(s,a) = \"future reward\" as a dictionnary:\n",
    "    # Q(s) = [ reward per action ]\n",
    "    # either as a table (or function approximation, next notebook)    \n",
    "    # stores the number of actions of the agent needed to add a new output \n",
    "\n",
    "    def __init__(self, env):       \n",
    "        self.nactions = env.action_space.n\n",
    "        self.f = {}\n",
    "        \n",
    "    def predict(self, s, a=None):\n",
    "        if s not in self.f:\n",
    "             self.f[s] = [0]*self.nactions       \n",
    "        return self.f[s] if a is None else self.f[s][a]\n",
    "    \n",
    "    def update(self, s, a, y):\n",
    "        self.f[s][a] = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************\n",
    "# Q Approximation with Non-Linear approximators with Deep Neural Networks\n",
    "# Requires tensorflow as the neural net is defined with it\n",
    "\n",
    "class DQN_function():\n",
    "    # stores the Q_function Q(s,a) = \"future reward\" but in fact Q(s) = [ f. reward per action ]\n",
    "    # as function approximation with a Neural Network\n",
    "\n",
    "    def __init__(self, env, params_qfunc=None):\n",
    "        from keras.models import Sequential\n",
    "        from keras.optimizers import Adam\n",
    "        from keras.layers import Dense\n",
    "\n",
    "        self.learning_rate = 0.001\n",
    "        if(params_qfunc is not None):\n",
    "            if(\"learning_rate\" in params_qfunc):\n",
    "                self.learning_rate = params_qfunc[\"learning_rate\"]\n",
    "\n",
    "        obs = env.reset() \n",
    "        if env.num_agents > 1:\n",
    "            obs = obs[0]\n",
    "        \n",
    "        self.input_shape = obs.flatten().shape[0]\n",
    "\n",
    "        print(\"DQN_function init  obs_space.shape\", env.observation_space.shape, \"   used: \", self.input_shape)\n",
    "        self.nactions = env.action_space.n\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(8, input_shape=(self.input_shape,), activation=\"relu\"))\n",
    "        self.model.add(Dense(16, activation=\"relu\"))\n",
    "        self.model.add(Dense(self.nactions, activation=\"linear\"))\n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=self.learning_rate))\n",
    "        self.model.summary()\n",
    "\n",
    "    def predict(self, s):\n",
    "        s_flat = s.flatten()\n",
    "        s_batch = np.reshape(s, [1, s_flat.shape[0]])\n",
    "        return self.model.predict(s_batch)[0]\n",
    "\n",
    "    def update(self, s, a, y):\n",
    "        s_flat = s.flatten()\n",
    "        s_batch = np.reshape(s, [1, s_flat.shape[0]])   # calling predict was causing\n",
    "        q_values = self.model.predict(s_batch)[0]       # bad inheritance behavior\n",
    "        q_values[a] = y\n",
    "        q_values_batch = np.reshape(q_values, [1, self.nactions])\n",
    "        self.model.fit(s_batch, q_values_batch, verbose=0)\n",
    "\n",
    "    def update_batch(self, states, targets):\n",
    "        self.model.train_on_batch(states, targets)\n",
    "\n",
    "    def size(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info(n, i_episode, num_episodes, epsilon, sum_rew, Q=None, steps=None):\n",
    "    if(i_episode == 0): \n",
    "        print(str(i_episode),\" Starting learning running \",num_episodes,\" episodes plotting every \", int(n),\" episodes\")  \n",
    "        return \n",
    "    if(epsilon < 0):\n",
    "        print(i_episode,\"   Mean Reward: \",sum_rew)\n",
    "    else:\n",
    "        str_steps = \"\" if steps is None else \" Steps: \"+str(steps)\n",
    "        str_Q = \"\" if Q is None else \" Q states: \"+str(Q.size())\n",
    "        print(i_episode, \" epsilon: %.1f\"%epsilon, \"   Mean Reward: %.1f\"%sum_rew, str_steps, str_Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(env, learning_rate=0.3, discount_factor=0.9, num_episodes=5000, num_episodes_end=4000, Q_function_class=Q_function):   \n",
    "    np.random.seed() \n",
    "    env.history = {\"episode_rew\":[], \"episode_steps\":[]}\n",
    "    Q =  Q_function_class(env)\n",
    "    mean_rew, mean_steps = 0, 0\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        epsilon = max(1 - (1/num_episodes_end) * i_episode, 0)        \n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        sum_reward, steps = 0, 0\n",
    "        while not done:\n",
    "            q_values = Q.predict(state)\n",
    "            if np.random.uniform() < epsilon:\n",
    "                action = np.random.randint(env.nactions)   # explore\n",
    "            else:\n",
    "                action = np.argmax(q_values)  # exploit\n",
    "\n",
    "            state_new, reward, done, _ = env.step(action)\n",
    "            sum_reward += reward\n",
    "            td_error = reward + discount_factor*np.max(Q.predict(state_new)) - q_values[action]\n",
    "            td_target = q_values[action] + learning_rate*td_error\n",
    "            Q.update(state,action,td_target)\n",
    "            state = state_new\n",
    "            steps += 1\n",
    "\n",
    "        env.history[\"episode_rew\"] += [sum_reward]\n",
    "        env.history[\"episode_steps\"] += [steps]\n",
    "        mean_rew += sum_reward\n",
    "        mean_steps += steps\n",
    "        \n",
    "        n = num_episodes / 50\n",
    "        if (i_episode+1) % n == 0:\n",
    "            print_info(n, i_episode, num_episodes, epsilon, mean_rew/n, steps=mean_steps/n, Q=Q)\n",
    "            mean_rew, mean_steps = 0, 0\n",
    "\n",
    "    env.close()\n",
    "    return Q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default number of agents: 1\n",
      "-allo-g-1ag Discrete Action Space with Discrete(4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf = {\"name\": \"FinalGrid\", \"rows\": 5, \"cols\": 5,\n",
    "        \"food\": 8,\n",
    "        \"action_mode\": ACT_MODE.ALLOCENTRIC,\n",
    "        \"obs_mode\": OBS_MODE.GLOBAL,\n",
    "        \"term_mode\": \"empty\",\n",
    "        \"max_steps\": 50,\n",
    "        }\n",
    "\n",
    "env = Environment(conf)\n",
    "\n",
    "env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN_function init  obs_space.shape (2, 5, 5)    used:  50\n",
      "WARNING:tensorflow:From /Users/marti/miniconda3/envs/insoco/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8)                 408       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 620\n",
      "Trainable params: 620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /Users/marti/miniconda3/envs/insoco/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "199  epsilon: 1.0    Mean Reward: 4.4  Steps: 49.565  Q states: 0\n",
      "399  epsilon: 1.0    Mean Reward: 4.4  Steps: 49.5  Q states: 0\n",
      "599  epsilon: 0.9    Mean Reward: 4.5  Steps: 49.74  Q states: 0\n",
      "799  epsilon: 0.9    Mean Reward: 4.5  Steps: 49.84  Q states: 0\n",
      "999  epsilon: 0.9    Mean Reward: 4.8  Steps: 49.665  Q states: 0\n",
      "1199  epsilon: 0.9    Mean Reward: 4.9  Steps: 49.84  Q states: 0\n",
      "1399  epsilon: 0.8    Mean Reward: 5.0  Steps: 49.47  Q states: 0\n",
      "1599  epsilon: 0.8    Mean Reward: 5.2  Steps: 49.575  Q states: 0\n",
      "1799  epsilon: 0.8    Mean Reward: 5.1  Steps: 49.47  Q states: 0\n",
      "1999  epsilon: 0.8    Mean Reward: 5.1  Steps: 49.38  Q states: 0\n",
      "2199  epsilon: 0.8    Mean Reward: 5.1  Steps: 49.12  Q states: 0\n",
      "2399  epsilon: 0.7    Mean Reward: 5.3  Steps: 49.42  Q states: 0\n",
      "2599  epsilon: 0.7    Mean Reward: 5.4  Steps: 49.185  Q states: 0\n",
      "2799  epsilon: 0.7    Mean Reward: 5.5  Steps: 49.115  Q states: 0\n",
      "2999  epsilon: 0.7    Mean Reward: 5.6  Steps: 48.405  Q states: 0\n",
      "3199  epsilon: 0.6    Mean Reward: 5.6  Steps: 48.6  Q states: 0\n",
      "3399  epsilon: 0.6    Mean Reward: 5.6  Steps: 48.525  Q states: 0\n",
      "3599  epsilon: 0.6    Mean Reward: 5.7  Steps: 48.385  Q states: 0\n",
      "3799  epsilon: 0.6    Mean Reward: 5.8  Steps: 47.965  Q states: 0\n",
      "3999  epsilon: 0.6    Mean Reward: 5.8  Steps: 48.41  Q states: 0\n",
      "4199  epsilon: 0.5    Mean Reward: 5.6  Steps: 48.48  Q states: 0\n",
      "4399  epsilon: 0.5    Mean Reward: 5.6  Steps: 48.955  Q states: 0\n",
      "4599  epsilon: 0.5    Mean Reward: 5.8  Steps: 48.485  Q states: 0\n",
      "4799  epsilon: 0.5    Mean Reward: 5.7  Steps: 48.12  Q states: 0\n",
      "4999  epsilon: 0.4    Mean Reward: 5.6  Steps: 48.185  Q states: 0\n",
      "5199  epsilon: 0.4    Mean Reward: 5.5  Steps: 48.75  Q states: 0\n",
      "5399  epsilon: 0.4    Mean Reward: 5.9  Steps: 47.695  Q states: 0\n",
      "5599  epsilon: 0.4    Mean Reward: 5.7  Steps: 48.87  Q states: 0\n",
      "5799  epsilon: 0.4    Mean Reward: 6.0  Steps: 46.975  Q states: 0\n",
      "5999  epsilon: 0.3    Mean Reward: 5.4  Steps: 48.11  Q states: 0\n",
      "6199  epsilon: 0.3    Mean Reward: 5.5  Steps: 48.285  Q states: 0\n",
      "6399  epsilon: 0.3    Mean Reward: 5.8  Steps: 47.11  Q states: 0\n",
      "6599  epsilon: 0.3    Mean Reward: 5.3  Steps: 48.54  Q states: 0\n",
      "6799  epsilon: 0.2    Mean Reward: 5.5  Steps: 48.09  Q states: 0\n",
      "6999  epsilon: 0.2    Mean Reward: 5.1  Steps: 48.275  Q states: 0\n",
      "7199  epsilon: 0.2    Mean Reward: 5.4  Steps: 48.48  Q states: 0\n",
      "7399  epsilon: 0.2    Mean Reward: 5.3  Steps: 48.63  Q states: 0\n",
      "7599  epsilon: 0.2    Mean Reward: 5.5  Steps: 48.555  Q states: 0\n",
      "7799  epsilon: 0.1    Mean Reward: 5.5  Steps: 48.295  Q states: 0\n",
      "7999  epsilon: 0.1    Mean Reward: 5.5  Steps: 47.16  Q states: 0\n",
      "8199  epsilon: 0.1    Mean Reward: 5.5  Steps: 47.465  Q states: 0\n",
      "8399  epsilon: 0.1    Mean Reward: 5.1  Steps: 48.455  Q states: 0\n",
      "8599  epsilon: 0.0    Mean Reward: 5.2  Steps: 47.85  Q states: 0\n",
      "8799  epsilon: 0.0    Mean Reward: 5.4  Steps: 48.025  Q states: 0\n",
      "8999  epsilon: 0.0    Mean Reward: 6.1  Steps: 43.955  Q states: 0\n",
      "9199  epsilon: 0.0    Mean Reward: 6.0  Steps: 44.52  Q states: 0\n",
      "9399  epsilon: 0.0    Mean Reward: 5.9  Steps: 45.165  Q states: 0\n",
      "9599  epsilon: 0.0    Mean Reward: 6.1  Steps: 45.25  Q states: 0\n",
      "9799  epsilon: 0.0    Mean Reward: 6.3  Steps: 44.435  Q states: 0\n",
      "9999  epsilon: 0.0    Mean Reward: 6.1  Steps: 43.685  Q states: 0\n"
     ]
    }
   ],
   "source": [
    "params_qlearn = {\"learning_rate\":0.1, \"discount_factor\":0.95, \"num_episodes\":10000, \"num_episodes_end\":9000}\n",
    "q_func = Q_learning(env, **params_qlearn, Q_function_class=DQN_function)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
