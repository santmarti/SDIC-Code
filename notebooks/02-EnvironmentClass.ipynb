{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Colab uncomment the following lines:\n",
    "# !git clone https://github.com/santmarti/SDIC-Code.git\n",
    "# %cd SDIC-Code/notebooks\n",
    "from Environment import Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment class defined in file Environment.py \n",
    "The class lets you create any OpenAI gym environment and also some others that we added. Here follow some valid names for environments instantiations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Simplest Example of Sutton's talk.     file:  suttonsimplest.py\n",
    "You only know that you have two possible actions: 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting default max_steps per episode: 100000\n",
      "SuttonSimplest--1ag Discrete Action Space with Discrete(2)\n",
      "Ini State: 0\n"
     ]
    }
   ],
   "source": [
    "env = Environment(\"SuttonSimplest\")\n",
    "print(\"Ini State: %d\"%env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 0 Reward 10\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(0)\n",
    "print(\"State %d Reward %d\" % (state, reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_random():\n",
    "    return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 980\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "sum_rew = 0\n",
    "for _ in range(100):\n",
    "    state, reward, _, _ = env.step(np.random.randint(2))\n",
    "    sum_rew += reward\n",
    "print(\"Total reward:\", sum_rew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Total reward: 1400\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print(state)\n",
    "sum_rew = 0\n",
    "for _ in range(100):\n",
    "    action = \n",
    "    state, reward, _, _ = env.step(action)\n",
    "    sum_rew += reward\n",
    "print(\"Total reward:\", sum_rew)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
