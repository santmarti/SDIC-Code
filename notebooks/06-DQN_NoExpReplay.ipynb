{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this notebook you need to install tensorflow and keras\n",
    "Recommended versions are: <BR>\n",
    "-tensorflow 1.15.5 <BR>\n",
    "-keras 2.3 <BR>\n",
    "They are 40x times faster than tensorflow 2.8 <BR>\n",
    "    \n",
    "How to install: <BR>\n",
    "`pip install tensorflow==1.15.5` <BR>\n",
    "`pip install keras==2.3`    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os; \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "from Environment import Environment, ACT_MODE, OBS_MODE\n",
    "from Plotting import plotQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_function():  \n",
    "    # stores the Q_function Q(s,a) = \"future reward\" as a dictionnary:\n",
    "    # Q(s) = [ reward per action ]\n",
    "    # either as a table (or function approximation, next notebook)    \n",
    "    # stores the number of actions of the agent needed to add a new output \n",
    "\n",
    "    def __init__(self, env):       \n",
    "        self.nactions = env.action_space.n\n",
    "        self.f = {}\n",
    "        \n",
    "    def predict(self, s, a=None):\n",
    "        if s not in self.f:\n",
    "             self.f[s] = [0]*self.nactions       \n",
    "        return self.f[s] if a is None else self.f[s][a]\n",
    "    \n",
    "    def update(self, s, a, y):\n",
    "        self.f[s][a] = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************\n",
    "# Q Approximation with Non-Linear approximators with Deep Neural Networks\n",
    "# Requires tensorflow as the neural net is defined with it\n",
    "\n",
    "class DQN_function():\n",
    "    # stores the Q_function Q(s,a) = \"future reward\" but in fact Q(s) = [ f. reward per action ]\n",
    "    # as function approximation with a Neural Network\n",
    "\n",
    "    def __init__(self, env, params_qfunc=None):\n",
    "        import tensorflow as tf\n",
    "        from keras.models import Sequential\n",
    "        if tf.version.VERSION[0] == '1':\n",
    "            from keras.optimizers import Adam\n",
    "            tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "        else:\n",
    "            from tf.keras.optimizers import Adam\n",
    "            tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "            \n",
    "        from keras.layers import Dense\n",
    "\n",
    "        self.learning_rate = 0.001\n",
    "        if(params_qfunc is not None):\n",
    "            if(\"learning_rate\" in params_qfunc):\n",
    "                self.learning_rate = params_qfunc[\"learning_rate\"]\n",
    "\n",
    "        obs = env.reset() \n",
    "        if env.num_agents > 1:\n",
    "            obs = obs[0]\n",
    "        \n",
    "        self.input_shape = obs.flatten().shape[0]\n",
    "\n",
    "        print(\"DQN_function init obs_space.shape\", env.observation_space.shape, \"   used: \", self.input_shape)\n",
    "        self.nactions = env.action_space.n\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(8, input_shape=(self.input_shape,), activation=\"relu\"))\n",
    "        self.model.add(Dense(16, activation=\"relu\"))\n",
    "        self.model.add(Dense(self.nactions, activation=\"linear\"))\n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        self.model.summary()\n",
    "\n",
    "    def predict(self, s):\n",
    "        s_flat = s.flatten()\n",
    "        s_batch = np.reshape(s, [1, s_flat.shape[0]])\n",
    "        return self.model.predict(s_batch)[0]\n",
    "\n",
    "    def update(self, s, a, y):\n",
    "        s_flat = s.flatten()\n",
    "        s_batch = np.reshape(s, [1, s_flat.shape[0]])   \n",
    "        q_values = self.model.predict(s_batch)[0]       \n",
    "        q_values[a] = y\n",
    "        q_values_batch = np.reshape(q_values, [1, self.nactions])\n",
    "        self.model.fit(s_batch, q_values_batch, verbose=0)\n",
    "\n",
    "    def update_batch(self, states, targets):\n",
    "        self.model.train_on_batch(states, targets)\n",
    "\n",
    "    def size(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info(n, i_episode, num_episodes, epsilon, sum_rew, Q=None, steps=None):\n",
    "    if(i_episode == 0): \n",
    "        print(str(i_episode),\" Starting learning running \",num_episodes,\" episodes plotting every \", int(n),\" episodes\")  \n",
    "        return \n",
    "    if(epsilon < 0):\n",
    "        print(i_episode,\"   Mean Reward: \",sum_rew)\n",
    "    else:\n",
    "        str_steps = \"\" if steps is None else \" Steps: \"+str(steps)\n",
    "        str_Q = \"\" if Q is None else \" Q states: \"+str(Q.size())\n",
    "        print(i_episode, \" epsilon: %.1f\"%epsilon, \"   Mean Reward: %.1f\"%sum_rew, str_steps, str_Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(env, learning_rate=0.3, discount_factor=0.9, num_episodes=5000, num_episodes_end=4000, Q_function_class=Q_function):   \n",
    "    np.random.seed()\n",
    "    env.history = {\"episode_rew\":[], \"episode_steps\":[]}\n",
    "    Q =  Q_function_class(env)\n",
    "    mean_rew, mean_steps = 0, 0\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        epsilon = max(1 - (1/num_episodes_end) * i_episode, 0)        \n",
    "        state = env.reset()\n",
    "        \n",
    "        done = False\n",
    "        sum_reward, steps = 0, 0\n",
    "        while not done:\n",
    "            q_values = Q.predict(state)\n",
    "            if np.random.uniform() < epsilon:\n",
    "                action = np.random.randint(env.nactions)   # explore\n",
    "            else:\n",
    "                action = np.argmax(q_values)  # exploit\n",
    "\n",
    "            state_new, reward, done, _ = env.step(action)\n",
    "\n",
    "            sum_reward += reward\n",
    "            td_error = reward + discount_factor*np.max(Q.predict(state_new)) - q_values[action]\n",
    "            td_target = q_values[action] + learning_rate*td_error\n",
    "            Q.update(state,action,td_target)\n",
    "            state = state_new\n",
    "            steps += 1\n",
    "\n",
    "        env.history[\"episode_rew\"] += [sum_reward]\n",
    "        env.history[\"episode_steps\"] += [steps]\n",
    "        mean_rew += sum_reward\n",
    "        mean_steps += steps\n",
    "        \n",
    "        n = num_episodes / 50\n",
    "        if (i_episode+1) % n == 0:\n",
    "            print_info(n, i_episode, num_episodes, epsilon, mean_rew/n, steps=mean_steps/n, Q=Q)\n",
    "            mean_rew, mean_steps = 0, 0\n",
    "\n",
    "    env.close()\n",
    "    return Q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default number of agents: 1\n",
      "-allo-g-1ag Discrete Action Space with Discrete(4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf = {\"name\": \"FinalGrid\", \"rows\": 6, \"cols\": 6,\n",
    "        \"food\": 8,\n",
    "        \"action_mode\": ACT_MODE.ALLOCENTRIC,\n",
    "        \"obs_mode\": OBS_MODE.GLOBAL,\n",
    "        \"term_mode\": \"empty\",\n",
    "        \"max_steps\": 50,\n",
    "        }\n",
    "\n",
    "env = Environment(conf)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN_function init obs_space.shape (2, 6, 6)    used:  72\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8)                 584       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 796\n",
      "Trainable params: 796\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "99  epsilon: 1.0    Mean Reward: 3.1  Steps: 50.0  Q states: 0\n",
      "199  epsilon: 1.0    Mean Reward: 3.3  Steps: 49.99  Q states: 0\n",
      "299  epsilon: 0.9    Mean Reward: 3.5  Steps: 50.0  Q states: 0\n",
      "399  epsilon: 0.9    Mean Reward: 3.5  Steps: 50.0  Q states: 0\n",
      "499  epsilon: 0.9    Mean Reward: 3.4  Steps: 50.0  Q states: 0\n",
      "599  epsilon: 0.9    Mean Reward: 3.3  Steps: 50.0  Q states: 0\n",
      "699  epsilon: 0.8    Mean Reward: 3.4  Steps: 50.0  Q states: 0\n",
      "799  epsilon: 0.8    Mean Reward: 3.7  Steps: 49.92  Q states: 0\n",
      "899  epsilon: 0.8    Mean Reward: 3.4  Steps: 50.0  Q states: 0\n",
      "999  epsilon: 0.8    Mean Reward: 3.6  Steps: 49.84  Q states: 0\n",
      "1099  epsilon: 0.8    Mean Reward: 3.6  Steps: 49.98  Q states: 0\n",
      "1199  epsilon: 0.7    Mean Reward: 3.4  Steps: 50.0  Q states: 0\n",
      "1299  epsilon: 0.7    Mean Reward: 3.6  Steps: 50.0  Q states: 0\n",
      "1399  epsilon: 0.7    Mean Reward: 3.3  Steps: 50.0  Q states: 0\n",
      "1499  epsilon: 0.7    Mean Reward: 3.5  Steps: 50.0  Q states: 0\n",
      "1599  epsilon: 0.6    Mean Reward: 3.5  Steps: 50.0  Q states: 0\n",
      "1699  epsilon: 0.6    Mean Reward: 3.6  Steps: 50.0  Q states: 0\n",
      "1799  epsilon: 0.6    Mean Reward: 3.3  Steps: 49.99  Q states: 0\n",
      "1899  epsilon: 0.6    Mean Reward: 3.4  Steps: 50.0  Q states: 0\n",
      "1999  epsilon: 0.6    Mean Reward: 3.4  Steps: 50.0  Q states: 0\n",
      "2099  epsilon: 0.5    Mean Reward: 3.8  Steps: 49.87  Q states: 0\n",
      "2199  epsilon: 0.5    Mean Reward: 3.6  Steps: 50.0  Q states: 0\n",
      "2299  epsilon: 0.5    Mean Reward: 3.7  Steps: 50.0  Q states: 0\n",
      "2399  epsilon: 0.5    Mean Reward: 3.5  Steps: 50.0  Q states: 0\n",
      "2499  epsilon: 0.4    Mean Reward: 3.6  Steps: 50.0  Q states: 0\n",
      "2599  epsilon: 0.4    Mean Reward: 3.7  Steps: 50.0  Q states: 0\n",
      "2699  epsilon: 0.4    Mean Reward: 3.6  Steps: 49.98  Q states: 0\n",
      "2799  epsilon: 0.4    Mean Reward: 3.3  Steps: 49.93  Q states: 0\n",
      "2899  epsilon: 0.4    Mean Reward: 2.9  Steps: 50.0  Q states: 0\n",
      "2999  epsilon: 0.3    Mean Reward: 3.3  Steps: 49.9  Q states: 0\n",
      "3099  epsilon: 0.3    Mean Reward: 3.4  Steps: 50.0  Q states: 0\n",
      "3199  epsilon: 0.3    Mean Reward: 3.4  Steps: 49.9  Q states: 0\n",
      "3299  epsilon: 0.3    Mean Reward: 3.3  Steps: 50.0  Q states: 0\n",
      "3399  epsilon: 0.2    Mean Reward: 3.0  Steps: 50.0  Q states: 0\n",
      "3499  epsilon: 0.2    Mean Reward: 3.1  Steps: 50.0  Q states: 0\n",
      "3599  epsilon: 0.2    Mean Reward: 2.9  Steps: 49.99  Q states: 0\n",
      "3699  epsilon: 0.2    Mean Reward: 2.9  Steps: 50.0  Q states: 0\n",
      "3799  epsilon: 0.2    Mean Reward: 2.9  Steps: 50.0  Q states: 0\n",
      "3899  epsilon: 0.1    Mean Reward: 2.7  Steps: 50.0  Q states: 0\n",
      "3999  epsilon: 0.1    Mean Reward: 2.8  Steps: 50.0  Q states: 0\n",
      "4099  epsilon: 0.1    Mean Reward: 2.3  Steps: 50.0  Q states: 0\n",
      "4199  epsilon: 0.1    Mean Reward: 2.1  Steps: 50.0  Q states: 0\n",
      "4299  epsilon: 0.0    Mean Reward: 2.2  Steps: 50.0  Q states: 0\n",
      "4399  epsilon: 0.0    Mean Reward: 1.6  Steps: 50.0  Q states: 0\n",
      "4499  epsilon: 0.0    Mean Reward: 1.6  Steps: 50.0  Q states: 0\n",
      "4599  epsilon: 0.0    Mean Reward: 1.7  Steps: 50.0  Q states: 0\n",
      "4699  epsilon: 0.0    Mean Reward: 1.5  Steps: 50.0  Q states: 0\n",
      "4799  epsilon: 0.0    Mean Reward: 1.9  Steps: 50.0  Q states: 0\n",
      "4899  epsilon: 0.0    Mean Reward: 1.9  Steps: 50.0  Q states: 0\n",
      "4999  epsilon: 0.0    Mean Reward: 1.7  Steps: 50.0  Q states: 0\n"
     ]
    }
   ],
   "source": [
    "n_epi = 5000\n",
    "params_qlearn = {\"learning_rate\":0.08, \"discount_factor\":0.98, \"num_episodes\":n_epi, \"num_episodes_end\":n_epi-500}\n",
    "q_func = Q_learning(env, **params_qlearn, Q_function_class=DQN_function)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
