{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from Environment import Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment class defined in file Environment.py \n",
    "The class lets you create any OpenAI gym environment and also some others that we added. Here follow some valid names for environments instantiations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Simplest Example of Sutton's talk.     file:  suttonsimplest.py\n",
    "You only know that you have two possible actions: 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting default max_steps per episode: 100000\n",
      "SuttonSimplest--1ag Discrete Action Space with Discrete(2)\n",
      "Ini State: 0\n"
     ]
    }
   ],
   "source": [
    "env = Environment(\"SuttonSimplest\")\n",
    "print(\"Ini State: %d\"%env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 1 Reward -10\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(1)\n",
    "print(\"State %d Reward %d\" % (state, reward))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
