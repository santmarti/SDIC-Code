{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this notebook you need to install tensorflow and keras\n",
    "Recommended versions are: <BR>\n",
    "-tensorflow 1.15.5 <BR>\n",
    "-keras 2.3 <BR>\n",
    "They are 40x times faster than tensorflow 2.8 <BR>\n",
    "    \n",
    "How to install: <BR>\n",
    "`pip install tensorflow==1.15.5` <BR>\n",
    "`pip install keras==2.3`    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os; \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "from Environment import Environment, ACT_MODE, OBS_MODE\n",
    "from Plotting import plotQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_function():  \n",
    "    # stores the Q_function Q(s,a) = \"future reward\" as a dictionnary:\n",
    "    # Q(s) = [ reward per action ]\n",
    "    # either as a table (or function approximation, next notebook)    \n",
    "    # stores the number of actions of the agent needed to add a new output \n",
    "\n",
    "    def __init__(self, env):       \n",
    "        self.nactions = env.action_space.n\n",
    "        self.f = {}\n",
    "        \n",
    "    def predict(self, s, a=None):\n",
    "        if s not in self.f:\n",
    "             self.f[s] = [0]*self.nactions       \n",
    "        return self.f[s] if a is None else self.f[s][a]\n",
    "    \n",
    "    def update(self, s, a, y):\n",
    "        self.f[s][a] = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************\n",
    "# Q Approximation with Non-Linear approximators with Deep Neural Networks\n",
    "# Requires tensorflow as the neural net is defined with it\n",
    "\n",
    "class DQN_function():\n",
    "    # stores the Q_function Q(s,a) = \"future reward\" but in fact Q(s) = [ f. reward per action ]\n",
    "    # as function approximation with a Neural Network\n",
    "\n",
    "    def __init__(self, env, params_qfunc=None):\n",
    "        import tensorflow as tf\n",
    "        from keras.models import Sequential\n",
    "        if tf.version.VERSION[0] == '1':\n",
    "            from keras.optimizers import Adam\n",
    "            tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "        else:\n",
    "            from tf.keras.optimizers import Adam\n",
    "            tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "            \n",
    "        from keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "        self.learning_rate = 0.001\n",
    "        if(params_qfunc is not None):\n",
    "            if(\"learning_rate\" in params_qfunc):\n",
    "                self.learning_rate = params_qfunc[\"learning_rate\"]\n",
    "                \n",
    "        obs = env.reset() \n",
    "        if env.num_agents > 1:\n",
    "            obs = obs[0]\n",
    "\n",
    "        self.bFlatten =  len(obs.shape) < 3\n",
    "        if self.bFlatten: self.input_shape = obs.flatten().shape[0]\n",
    "        else: self.input_shape = obs.shape\n",
    "\n",
    "        print(\"DQN_function init obs_space.shape\", env.observation_space.shape, \"   used: \", self.input_shape)\n",
    "        self.nactions = env.action_space.n\n",
    "\n",
    "        self.model = Sequential()\n",
    "        if self.bFlatten: self.model.add(Dense(8, input_shape=(self.input_shape,), activation=\"relu\"))\n",
    "        else: \n",
    "            self.model.add(Conv2D(32, kernel_size=(2, 2), activation=\"relu\", input_shape=self.input_shape, data_format='channels_first'))\n",
    " \n",
    "        self.model.add(Dense(16, activation=\"relu\"))\n",
    "        if not self.bFlatten: self.model.add(Flatten())\n",
    "        self.model.add(Dense(self.nactions, activation=\"linear\"))\n",
    "\n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        self.model.summary()\n",
    "\n",
    "    def predict(self, s):\n",
    "        if self.bFlatten:\n",
    "            s_flat = s.flatten()\n",
    "            s_batch = np.reshape(s, [1, s_flat.shape[0]])\n",
    "        else:\n",
    "            s_batch = np.reshape(s, [1] +  list(self.input_shape))\n",
    "        return self.model.predict(s_batch)[0]\n",
    "\n",
    "    def update(self, s, a, y):\n",
    "        if self.bFlatten:\n",
    "            s_flat = s.flatten()\n",
    "            s_batch = np.reshape(s, [1, s_flat.shape[0]])   \n",
    "        else:\n",
    "            s_batch = np.reshape(s, [1] +  list(self.input_shape))    \n",
    "            \n",
    "        q_values = self.model.predict(s_batch)[0]\n",
    "        q_values[a] = y\n",
    "        q_values_batch = np.reshape(q_values, [1, self.nactions])\n",
    "        self.model.fit(s_batch, q_values_batch, verbose=0)\n",
    "\n",
    "    def update_batch(self, states, targets):\n",
    "        self.model.train_on_batch(states, targets)\n",
    "\n",
    "    def size(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info(n, i_episode, num_episodes, epsilon, sum_rew, Q=None, steps=None):\n",
    "    if(i_episode == 0): \n",
    "        print(str(i_episode),\" Starting learning running \",num_episodes,\" episodes plotting every \", int(n),\" episodes\")  \n",
    "        return \n",
    "    if(epsilon < 0):\n",
    "        print(i_episode,\"   Mean Reward: \",sum_rew)\n",
    "    else:\n",
    "        str_steps = \"\" if steps is None else \" Steps: \"+str(steps)\n",
    "        str_Q = \"\" if Q is None else \" Q states: \"+str(Q.size())\n",
    "        print(i_episode, \" epsilon: %.1f\"%epsilon, \"   Mean Reward: %.1f\"%sum_rew, str_steps, str_Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(env, learning_rate=0.3, discount_factor=0.9, num_episodes=5000, num_episodes_end=4000, Q_function_class=Q_function):   \n",
    "    np.random.seed()\n",
    "    env.history = {\"episode_rew\":[], \"episode_steps\":[]}\n",
    "    Q =  Q_function_class(env)\n",
    "    mean_rew, mean_steps = 0, 0\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        epsilon = max(1 - (1/num_episodes_end) * i_episode, 0)        \n",
    "        state = env.reset()\n",
    "        \n",
    "        done = False\n",
    "        sum_reward, steps = 0, 0\n",
    "        while not done:\n",
    "            q_values = Q.predict(state)\n",
    "            if np.random.uniform() < epsilon:\n",
    "                action = np.random.randint(env.nactions)   # explore\n",
    "            else:\n",
    "                action = np.argmax(q_values)  # exploit\n",
    "\n",
    "            state_new, reward, done, _ = env.step(action)\n",
    "\n",
    "            sum_reward += reward\n",
    "            td_error = reward + discount_factor*np.max(Q.predict(state_new)) - q_values[action]\n",
    "            td_target = q_values[action] + learning_rate*td_error\n",
    "            Q.update(state,action,td_target)\n",
    "            state = state_new\n",
    "            steps += 1\n",
    "\n",
    "        env.history[\"episode_rew\"] += [sum_reward]\n",
    "        env.history[\"episode_steps\"] += [steps]\n",
    "        mean_rew += sum_reward\n",
    "        mean_steps += steps\n",
    "        \n",
    "        n = num_episodes / 50\n",
    "        if (i_episode+1) % n == 0:\n",
    "            print_info(n, i_episode, num_episodes, epsilon, mean_rew/n, steps=mean_steps/n, Q=Q)\n",
    "            mean_rew, mean_steps = 0, 0\n",
    "\n",
    "    env.close()\n",
    "    return Q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default number of agents: 1\n",
      "-allo-g-1ag Discrete Action Space with Discrete(4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf = {\"name\": \"FinalGrid\", \"rows\": 6, \"cols\": 6,\n",
    "        \"food\": 8,\n",
    "        \"action_mode\": ACT_MODE.ALLOCENTRIC,\n",
    "        \"obs_mode\": OBS_MODE.GLOBAL,\n",
    "        \"term_mode\": \"empty\",\n",
    "        \"max_steps\": 50,\n",
    "        }\n",
    "\n",
    "env = Environment(conf)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN_function init obs_space.shape (2, 6, 6)    used:  (2, 6, 6)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 5, 5)          288       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32, 5, 16)         96        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 10244     \n",
      "=================================================================\n",
      "Total params: 10,628\n",
      "Trainable params: 10,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "99  epsilon: 1.0    Mean Reward: 3.4  Steps: 49.84  Q states: 0\n",
      "199  epsilon: 1.0    Mean Reward: 3.2  Steps: 50.0  Q states: 0\n",
      "299  epsilon: 0.9    Mean Reward: 3.3  Steps: 50.0  Q states: 0\n",
      "399  epsilon: 0.9    Mean Reward: 3.5  Steps: 50.0  Q states: 0\n",
      "499  epsilon: 0.9    Mean Reward: 3.5  Steps: 50.0  Q states: 0\n",
      "599  epsilon: 0.9    Mean Reward: 3.6  Steps: 50.0  Q states: 0\n",
      "699  epsilon: 0.8    Mean Reward: 3.7  Steps: 50.0  Q states: 0\n",
      "799  epsilon: 0.8    Mean Reward: 3.1  Steps: 50.0  Q states: 0\n",
      "899  epsilon: 0.8    Mean Reward: 3.7  Steps: 50.0  Q states: 0\n",
      "999  epsilon: 0.8    Mean Reward: 3.3  Steps: 50.0  Q states: 0\n",
      "1099  epsilon: 0.8    Mean Reward: 3.5  Steps: 50.0  Q states: 0\n",
      "1199  epsilon: 0.7    Mean Reward: 3.4  Steps: 50.0  Q states: 0\n",
      "1299  epsilon: 0.7    Mean Reward: 3.7  Steps: 49.97  Q states: 0\n",
      "1399  epsilon: 0.7    Mean Reward: 3.6  Steps: 49.99  Q states: 0\n",
      "1499  epsilon: 0.7    Mean Reward: 3.7  Steps: 49.94  Q states: 0\n",
      "1599  epsilon: 0.6    Mean Reward: 3.8  Steps: 49.95  Q states: 0\n",
      "1699  epsilon: 0.6    Mean Reward: 4.4  Steps: 50.0  Q states: 0\n",
      "1799  epsilon: 0.6    Mean Reward: 4.6  Steps: 49.83  Q states: 0\n",
      "1899  epsilon: 0.6    Mean Reward: 5.2  Steps: 49.38  Q states: 0\n",
      "1999  epsilon: 0.6    Mean Reward: 5.2  Steps: 49.0  Q states: 0\n",
      "2099  epsilon: 0.5    Mean Reward: 5.4  Steps: 48.75  Q states: 0\n",
      "2199  epsilon: 0.5    Mean Reward: 5.6  Steps: 48.46  Q states: 0\n",
      "2299  epsilon: 0.5    Mean Reward: 6.0  Steps: 47.83  Q states: 0\n",
      "2399  epsilon: 0.5    Mean Reward: 5.9  Steps: 46.64  Q states: 0\n",
      "2499  epsilon: 0.4    Mean Reward: 5.9  Steps: 46.27  Q states: 0\n",
      "2599  epsilon: 0.4    Mean Reward: 5.9  Steps: 46.67  Q states: 0\n",
      "2699  epsilon: 0.4    Mean Reward: 6.1  Steps: 46.85  Q states: 0\n",
      "2799  epsilon: 0.4    Mean Reward: 6.4  Steps: 44.27  Q states: 0\n",
      "2899  epsilon: 0.4    Mean Reward: 6.6  Steps: 43.33  Q states: 0\n",
      "2999  epsilon: 0.3    Mean Reward: 6.7  Steps: 41.8  Q states: 0\n",
      "3099  epsilon: 0.3    Mean Reward: 6.4  Steps: 43.09  Q states: 0\n",
      "3199  epsilon: 0.3    Mean Reward: 6.8  Steps: 40.66  Q states: 0\n",
      "3299  epsilon: 0.3    Mean Reward: 6.8  Steps: 43.13  Q states: 0\n",
      "3399  epsilon: 0.2    Mean Reward: 6.9  Steps: 39.41  Q states: 0\n",
      "3499  epsilon: 0.2    Mean Reward: 7.0  Steps: 37.25  Q states: 0\n",
      "3599  epsilon: 0.2    Mean Reward: 6.8  Steps: 39.48  Q states: 0\n",
      "3699  epsilon: 0.2    Mean Reward: 7.0  Steps: 37.53  Q states: 0\n",
      "3799  epsilon: 0.2    Mean Reward: 7.1  Steps: 37.14  Q states: 0\n",
      "3899  epsilon: 0.1    Mean Reward: 7.1  Steps: 38.21  Q states: 0\n",
      "3999  epsilon: 0.1    Mean Reward: 7.3  Steps: 34.03  Q states: 0\n",
      "4099  epsilon: 0.1    Mean Reward: 7.2  Steps: 34.62  Q states: 0\n",
      "4199  epsilon: 0.1    Mean Reward: 6.5  Steps: 41.37  Q states: 0\n",
      "4299  epsilon: 0.0    Mean Reward: 7.2  Steps: 33.69  Q states: 0\n",
      "4399  epsilon: 0.0    Mean Reward: 7.4  Steps: 32.07  Q states: 0\n",
      "4499  epsilon: 0.0    Mean Reward: 7.0  Steps: 31.75  Q states: 0\n",
      "4599  epsilon: 0.0    Mean Reward: 7.3  Steps: 31.48  Q states: 0\n",
      "4699  epsilon: 0.0    Mean Reward: 7.4  Steps: 27.56  Q states: 0\n",
      "4799  epsilon: 0.0    Mean Reward: 7.6  Steps: 27.89  Q states: 0\n",
      "4899  epsilon: 0.0    Mean Reward: 7.5  Steps: 27.43  Q states: 0\n",
      "4999  epsilon: 0.0    Mean Reward: 7.3  Steps: 31.11  Q states: 0\n"
     ]
    }
   ],
   "source": [
    "n_epi = 5000\n",
    "params_qlearn = {\"learning_rate\":0.08, \"discount_factor\":0.98, \"num_episodes\":n_epi, \"num_episodes_end\":n_epi-500}\n",
    "q_func = Q_learning(env, **params_qlearn, Q_function_class=DQN_function)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
