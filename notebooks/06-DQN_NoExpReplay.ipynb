{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this notebook you need to install tensorflow and keras\n",
    "Recommended versions are: <BR>\n",
    "-tensorflow 1.15.5 <BR>\n",
    "-keras 2.3 <BR>\n",
    "They are 40x times faster than tensorflow 2.8 <BR>\n",
    "    \n",
    "How to install: <BR>\n",
    "`pip install tensorflow==1.15.5` <BR>\n",
    "`pip install keras==2.3`    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os; \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "from Environment import Environment, ACT_MODE, OBS_MODE\n",
    "from Plotting import plotQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_function():  \n",
    "    # stores the Q_function Q(s,a) = \"future reward\" as a dictionnary:\n",
    "    # Q(s) = [ reward per action ]\n",
    "    # either as a table (or function approximation, next notebook)    \n",
    "    # stores the number of actions of the agent needed to add a new output \n",
    "\n",
    "    def __init__(self, env):       \n",
    "        self.nactions = env.action_space.n\n",
    "        self.f = {}\n",
    "        \n",
    "    def predict(self, s, a=None):\n",
    "        if s not in self.f:\n",
    "             self.f[s] = [0]*self.nactions       \n",
    "        return self.f[s] if a is None else self.f[s][a]\n",
    "    \n",
    "    def update(self, s, a, y):\n",
    "        self.f[s][a] = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************\n",
    "# Q Approximation with Non-Linear approximators with Deep Neural Networks\n",
    "# Requires tensorflow as the neural net is defined with it\n",
    "\n",
    "class DQN_function():\n",
    "    # stores the Q_function Q(s,a) = \"future reward\" but in fact Q(s) = [ f. reward per action ]\n",
    "    # as function approximation with a Neural Network\n",
    "\n",
    "    def __init__(self, env, params_qfunc=None):\n",
    "        import tensorflow as tf\n",
    "        from keras.models import Sequential\n",
    "        if tf.version.VERSION[0] == '1':\n",
    "            from keras.optimizers import Adam\n",
    "            tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "        else:\n",
    "            from tf.keras.optimizers import Adam\n",
    "            tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "            \n",
    "        from keras.layers import Dense\n",
    "\n",
    "        self.learning_rate = 0.001\n",
    "        if(params_qfunc is not None):\n",
    "            if(\"learning_rate\" in params_qfunc):\n",
    "                self.learning_rate = params_qfunc[\"learning_rate\"]\n",
    "\n",
    "        obs = env.reset() \n",
    "        if env.num_agents > 1:\n",
    "            obs = obs[0]\n",
    "        \n",
    "        self.input_shape = obs.flatten().shape[0]\n",
    "\n",
    "        print(\"DQN_function init obs_space.shape\", env.observation_space.shape, \"   used: \", self.input_shape)\n",
    "        self.nactions = env.action_space.n\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(8, input_shape=(self.input_shape,), activation=\"relu\"))\n",
    "        self.model.add(Dense(16, activation=\"relu\"))\n",
    "        self.model.add(Dense(self.nactions, activation=\"linear\"))\n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        self.model.summary()\n",
    "\n",
    "    def predict(self, s):\n",
    "        s_flat = s.flatten()\n",
    "        s_batch = np.reshape(s, [1, s_flat.shape[0]])\n",
    "        return self.model.predict(s_batch)[0]\n",
    "\n",
    "    def update(self, s, a, y):\n",
    "        s_flat = s.flatten()\n",
    "        s_batch = np.reshape(s, [1, s_flat.shape[0]])   \n",
    "        q_values = self.model.predict(s_batch)[0]       \n",
    "        q_values[a] = y\n",
    "        q_values_batch = np.reshape(q_values, [1, self.nactions])\n",
    "        self.model.fit(s_batch, q_values_batch, verbose=0)\n",
    "\n",
    "    def update_batch(self, states, targets):\n",
    "        self.model.train_on_batch(states, targets)\n",
    "\n",
    "    def size(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info(n, i_episode, num_episodes, epsilon, sum_rew, Q=None, steps=None):\n",
    "    if(i_episode == 0): \n",
    "        print(str(i_episode),\" Starting learning running \",num_episodes,\" episodes plotting every \", int(n),\" episodes\")  \n",
    "        return \n",
    "    if(epsilon < 0):\n",
    "        print(i_episode,\"   Mean Reward: \",sum_rew)\n",
    "    else:\n",
    "        str_steps = \"\" if steps is None else \" Steps: \"+str(steps)\n",
    "        str_Q = \"\" if Q is None else \" Q states: \"+str(Q.size())\n",
    "        print(i_episode, \" epsilon: %.1f\"%epsilon, \"   Mean Reward: %.1f\"%sum_rew, str_steps, str_Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(env, learning_rate=0.3, discount_factor=0.9, num_episodes=5000, num_episodes_end=4000, Q_function_class=Q_function):   \n",
    "    np.random.seed()\n",
    "    env.history = {\"episode_rew\":[], \"episode_steps\":[]}\n",
    "    Q =  Q_function_class(env)\n",
    "    mean_rew, mean_steps = 0, 0\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        epsilon = max(1 - (1/num_episodes_end) * i_episode, 0)        \n",
    "        state = env.reset()\n",
    "        \n",
    "        done = False\n",
    "        sum_reward, steps = 0, 0\n",
    "        while not done:\n",
    "            q_values = Q.predict(state)\n",
    "            if np.random.uniform() < epsilon:\n",
    "                action = np.random.randint(env.nactions)   # explore\n",
    "            else:\n",
    "                action = np.argmax(q_values)  # exploit\n",
    "\n",
    "            state_new, reward, done, _ = env.step(action)\n",
    "\n",
    "            sum_reward += reward\n",
    "            td_error = reward + discount_factor*np.max(Q.predict(state_new)) - q_values[action]\n",
    "            td_target = q_values[action] + learning_rate*td_error\n",
    "            Q.update(state,action,td_target)\n",
    "            state = state_new\n",
    "            steps += 1\n",
    "\n",
    "        env.history[\"episode_rew\"] += [sum_reward]\n",
    "        env.history[\"episode_steps\"] += [steps]\n",
    "        mean_rew += sum_reward\n",
    "        mean_steps += steps\n",
    "        \n",
    "        n = num_episodes / 50\n",
    "        if (i_episode+1) % n == 0:\n",
    "            print_info(n, i_episode, num_episodes, epsilon, mean_rew/n, steps=mean_steps/n, Q=Q)\n",
    "            mean_rew, mean_steps = 0, 0\n",
    "\n",
    "    env.close()\n",
    "    return Q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default number of agents: 1\n",
      "-allo-local-3rad-1ag Discrete Action Space with Discrete(4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf = {\"name\": \"FinalGrid\", \"rows\": 6, \"cols\": 6,\n",
    "        \"food\": 8,\n",
    "        \"action_mode\": ACT_MODE.ALLOCENTRIC,\n",
    "        \"obs_mode\": OBS_MODE.LOCAL,\n",
    "        \"obs_radius\": 3,\n",
    "        \"term_mode\": \"empty\",\n",
    "        \"max_steps\": 50,\n",
    "        }\n",
    "\n",
    "env = Environment(conf)\n",
    "\n",
    "env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN_function init obs_space.shape (1, 7, 7)    used:  49\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8)                 400       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 612\n",
      "Trainable params: 612\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "99  epsilon: 1.0    Mean Reward: 3.5  Steps: 50.0  Q states: 0\n",
      "199  epsilon: 1.0    Mean Reward: 3.5  Steps: 50.0  Q states: 0\n",
      "299  epsilon: 1.0    Mean Reward: 3.5  Steps: 49.97  Q states: 0\n",
      "399  epsilon: 1.0    Mean Reward: 3.4  Steps: 49.98  Q states: 0\n",
      "499  epsilon: 0.9    Mean Reward: 3.2  Steps: 50.0  Q states: 0\n",
      "599  epsilon: 0.9    Mean Reward: 3.4  Steps: 49.92  Q states: 0\n",
      "699  epsilon: 0.9    Mean Reward: 3.3  Steps: 49.94  Q states: 0\n",
      "799  epsilon: 0.9    Mean Reward: 3.7  Steps: 49.77  Q states: 0\n",
      "899  epsilon: 0.9    Mean Reward: 3.7  Steps: 49.84  Q states: 0\n",
      "999  epsilon: 0.9    Mean Reward: 3.6  Steps: 50.0  Q states: 0\n",
      "1099  epsilon: 0.9    Mean Reward: 3.6  Steps: 50.0  Q states: 0\n",
      "1199  epsilon: 0.9    Mean Reward: 3.9  Steps: 49.82  Q states: 0\n",
      "1299  epsilon: 0.9    Mean Reward: 3.7  Steps: 49.41  Q states: 0\n",
      "1399  epsilon: 0.8    Mean Reward: 4.2  Steps: 50.0  Q states: 0\n",
      "1499  epsilon: 0.8    Mean Reward: 4.3  Steps: 49.8  Q states: 0\n",
      "1599  epsilon: 0.8    Mean Reward: 4.0  Steps: 49.9  Q states: 0\n",
      "1699  epsilon: 0.8    Mean Reward: 4.0  Steps: 50.0  Q states: 0\n",
      "1799  epsilon: 0.8    Mean Reward: 4.0  Steps: 49.92  Q states: 0\n",
      "1899  epsilon: 0.8    Mean Reward: 4.1  Steps: 49.98  Q states: 0\n",
      "1999  epsilon: 0.8    Mean Reward: 4.3  Steps: 49.76  Q states: 0\n",
      "2099  epsilon: 0.8    Mean Reward: 4.2  Steps: 49.99  Q states: 0\n",
      "2199  epsilon: 0.8    Mean Reward: 4.2  Steps: 49.74  Q states: 0\n",
      "2299  epsilon: 0.7    Mean Reward: 4.4  Steps: 49.14  Q states: 0\n",
      "2399  epsilon: 0.7    Mean Reward: 4.8  Steps: 49.42  Q states: 0\n",
      "2499  epsilon: 0.7    Mean Reward: 4.6  Steps: 49.65  Q states: 0\n",
      "2599  epsilon: 0.7    Mean Reward: 4.7  Steps: 49.35  Q states: 0\n",
      "2699  epsilon: 0.7    Mean Reward: 4.8  Steps: 48.7  Q states: 0\n",
      "2799  epsilon: 0.7    Mean Reward: 4.9  Steps: 48.96  Q states: 0\n",
      "2899  epsilon: 0.7    Mean Reward: 4.9  Steps: 49.3  Q states: 0\n",
      "2999  epsilon: 0.7    Mean Reward: 4.5  Steps: 49.94  Q states: 0\n",
      "3099  epsilon: 0.7    Mean Reward: 4.9  Steps: 49.23  Q states: 0\n",
      "3199  epsilon: 0.6    Mean Reward: 4.7  Steps: 49.71  Q states: 0\n",
      "3299  epsilon: 0.6    Mean Reward: 4.9  Steps: 48.73  Q states: 0\n",
      "3399  epsilon: 0.6    Mean Reward: 4.9  Steps: 49.23  Q states: 0\n",
      "3499  epsilon: 0.6    Mean Reward: 4.7  Steps: 49.46  Q states: 0\n",
      "3599  epsilon: 0.6    Mean Reward: 4.8  Steps: 49.65  Q states: 0\n",
      "3699  epsilon: 0.6    Mean Reward: 4.7  Steps: 49.33  Q states: 0\n",
      "3799  epsilon: 0.6    Mean Reward: 5.0  Steps: 49.12  Q states: 0\n",
      "3899  epsilon: 0.6    Mean Reward: 4.7  Steps: 49.44  Q states: 0\n",
      "3999  epsilon: 0.6    Mean Reward: 4.0  Steps: 49.24  Q states: 0\n",
      "4099  epsilon: 0.5    Mean Reward: 4.5  Steps: 49.31  Q states: 0\n",
      "4199  epsilon: 0.5    Mean Reward: 4.6  Steps: 49.35  Q states: 0\n",
      "4299  epsilon: 0.5    Mean Reward: 4.6  Steps: 49.55  Q states: 0\n",
      "4399  epsilon: 0.5    Mean Reward: 4.9  Steps: 49.46  Q states: 0\n",
      "4499  epsilon: 0.5    Mean Reward: 4.8  Steps: 49.0  Q states: 0\n",
      "4599  epsilon: 0.5    Mean Reward: 4.9  Steps: 48.82  Q states: 0\n",
      "4699  epsilon: 0.5    Mean Reward: 4.9  Steps: 49.01  Q states: 0\n",
      "4799  epsilon: 0.5    Mean Reward: 5.3  Steps: 48.58  Q states: 0\n",
      "4899  epsilon: 0.5    Mean Reward: 5.4  Steps: 48.54  Q states: 0\n",
      "4999  epsilon: 0.4    Mean Reward: 5.2  Steps: 47.99  Q states: 0\n"
     ]
    }
   ],
   "source": [
    "params_qlearn = {\"learning_rate\":0.1, \"discount_factor\":0.95, \"num_episodes\":5000, \"num_episodes_end\":9000}\n",
    "q_func = Q_learning(env, **params_qlearn, Q_function_class=DQN_function)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
