{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4ddd0ce",
   "metadata": {},
   "source": [
    "Immediate Reward Prediction (Stateless environment, only one single state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "daa917aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: {'L': 0, 'R': 0}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "A = ['L','R']\n",
    "Q = {'L':0, 'R':0}           # Q : A -> Value \n",
    "alfa = 0.1\n",
    "R = {'L':1, 'R':0}    # The reward function belongs to the environment and is invisible to the agent \n",
    "print(\"Q:\", Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "581e2b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'L'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "43199c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'L': 0.992144832788721, 'R': 0.0}\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    a = random.choice(A)\n",
    "    Q[a] = Q[a] + alfa*(R[a] - Q[a])\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a03ed57",
   "metadata": {},
   "source": [
    "Lets change the reward to the other arm of the T-Maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "5fc0a867",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = {'L':0, 'R':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "6b4dd8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done L   --> TD error:  -0.8929303495098488\n",
      "{'L': 0.8929303495098488, 'R': 0.1}\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1):\n",
    "    a = random.choice(A)\n",
    "    Q[a] = Q[a] + alfa*(R[a] - Q[a])\n",
    "    print(\"Done\", a, \"  --> TD error: \", R[a] - Q[a])\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e2a0ca",
   "metadata": {},
   "source": [
    "Environment with states and future discounted reward prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "d8a8b506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: {('A', 'L'): 0, ('A', 'R'): 0, ('B', 'L'): 0, ('B', 'R'): 0, ('C', 'L'): 0, ('C', 'R'): 0}\n",
      "R: {('A', 'L'): 0, ('A', 'R'): 0, ('B', 'L'): 1, ('B', 'R'): 0, ('C', 'L'): 0, ('C', 'R'): -10}\n"
     ]
    }
   ],
   "source": [
    "S = ['A','B','C']\n",
    "R = {}\n",
    "Q = {}\n",
    "for s in S:\n",
    "    for a in A:\n",
    "        Q[(s,a)] = 0\n",
    "        R[(s,a)] = 0\n",
    "\n",
    "R[('B','L')] = 1\n",
    "R[('C','R')] = -10\n",
    "gamma = 0.9\n",
    "\n",
    "print(\"Q:\",Q)\n",
    "print(\"R:\",R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "e93d6ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q Before: {('A', 'L'): 0.4240860395391629, ('A', 'R'): -4.225637961664771, ('B', 'L'): 0.9721871610556306, ('B', 'R'): 0.0, ('C', 'L'): 0.0, ('C', 'R'): -9.65663161797075}\n",
      "Was in A did action L\n",
      "Was in B did action R\n",
      "Q After: {('A', 'L'): 0.42542585783275, ('A', 'R'): -4.225637961664771, ('B', 'L'): 0.9721871610556306, ('B', 'R'): 0.0, ('C', 'L'): 0.0, ('C', 'R'): -9.65663161797075}\n"
     ]
    }
   ],
   "source": [
    "print(\"Q Before:\", Q)\n",
    "\n",
    "for _ in range(1):    #Â trials loop    \n",
    "    # one single trial\n",
    "    \n",
    "    # decision on first junction\n",
    "    S_current = 'A'\n",
    "    a = random.choice(A)\n",
    "    print(\"Was in\", S_current, \"did action\", a)\n",
    "    S_next = 'B' if a == 'L' else 'C'  # transition function from environment\n",
    "    \n",
    "    q_max_S_next = max([Q[(S_next, a_)] for a_ in A])\n",
    "    TD_error = R[(S_current,a)] + gamma*q_max_S_next - Q[(S_current,a)]\n",
    "    Q[(S_current,a)] = Q[(S_current,a)] + alfa*TD_error\n",
    "\n",
    "    S_current = S_next\n",
    "    # decision on second junction\n",
    "    a = random.choice(A)\n",
    "    print(\"Was in\", S_current, \"did action\", a)\n",
    "    Q[(S_current,a)] = Q[(S_current,a)] + alfa*(R[(S_current,a)] - Q[(S_current,a)])\n",
    "\n",
    "print(\"Q After:\", Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
